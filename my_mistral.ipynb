{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQgy41pmPWuCLxoD9Sk29U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniervs/small-mistral-like-llm/blob/main/my_mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorboard"
      ],
      "metadata": {
        "id": "LLdR0Yu5Jb3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "Gc342FEFJa4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "metadata": {
        "id": "K3yDeu-afA28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMXMku3pe2Vd"
      },
      "outputs": [],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install einops"
      ],
      "metadata": {
        "id": "SIr9CkmbfWdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Jnjayva9fC2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import einops\n",
        "import numpy as np\n",
        "from pydantic import BaseModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, DataProcessor, DataCollatorForLanguageModeling\n",
        "from collections import Counter\n",
        "from einops import reduce, repeat, rearrange, einsum\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "Id2Vj-4dfCY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device_name():\n",
        "    if torch.cuda.is_available():\n",
        "        return 'cuda'\n",
        "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        return 'mps'\n",
        "    return 'cpu'\n",
        "\n",
        "device = torch.device(get_device_name())\n",
        "device"
      ],
      "metadata": {
        "id": "E-4R3KbDO1vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n"
      ],
      "metadata": {
        "id": "AERLa6LXfEDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"HuggingFaceH4/no_robots\")"
      ],
      "metadata": {
        "id": "Uj111zxNe-nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train_sft']\n",
        "test_dataset = dataset['test_sft']\n",
        "train_dataset.shape, test_dataset.shape"
      ],
      "metadata": {
        "id": "s1owFoh4f-Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.features"
      ],
      "metadata": {
        "id": "gBf4qflOgQ34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "lxRPoYbegZZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_cnt = Counter(train_dataset['category'])\n",
        "plt.pie(category_cnt.values(), labels = category_cnt.keys(), autopct='%1.1f%%')\n",
        "plt.title(\"Distribution of Prompts' Categories\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kz6DU4WZgMaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "6qkhBcWtfF7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "ddYZRng9i1IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(datum: dict):\n",
        "    prompt = \"\\n\\n\".join([message[\"content\"] for message in datum['messages']])\n",
        "    tokens = tokenizer.encode(\n",
        "        text = prompt,\n",
        "        truncation = True,\n",
        "        padding = \"max_length\",\n",
        "        max_length=2048,\n",
        "        return_tensors = 'pt'\n",
        "    )[0]\n",
        "    return {'input_ids' : tokens}"
      ],
      "metadata": {
        "id": "ARWQFsqbh5WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize(train_dataset[0]))"
      ],
      "metadata": {
        "id": "2vMStRVhihk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = train_dataset.column_names\n",
        "train_tokens = train_dataset.map(tokenize, num_proc = 4, remove_columns = columns)\n",
        "test_tokens = test_dataset.map(tokenize, num_proc = 4, remove_columns = columns)\n",
        "\n",
        "train_tokens.shape"
      ],
      "metadata": {
        "id": "Lt8pdwP2kWSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders"
      ],
      "metadata": {
        "id": "QzJxQAazfH1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "8yK_zJKymBvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_tokens, shuffle = True, batch_size = 4, collate_fn = collator)\n",
        "test_loader = DataLoader(test_tokens, shuffle = False, batch_size = 4, collate_fn = collator)"
      ],
      "metadata": {
        "id": "3aIdH26Nm6P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "k8T7SSS1pTv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.keys()"
      ],
      "metadata": {
        "id": "FV30uAIWp1D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch['input_ids'][0][-10:]"
      ],
      "metadata": {
        "id": "xB4KAj2Ipc_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch['labels'][0][-10:]"
      ],
      "metadata": {
        "id": "aGiWhW_Jp0Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch['attention_mask']"
      ],
      "metadata": {
        "id": "oN79fN97ppU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "2OHLQHpnfI_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "800zgtmzsawp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelArguments(BaseModel):\n",
        "    dim: int = 1024 # originally, 4096\n",
        "    n_layers: int = 2 # originally, 32 (two layers are enough to see induction heads)\n",
        "    head_dim: int = 32 # originally 128\n",
        "    hidden_dim: int = 256 # originally 7 * 2048\n",
        "    n_heads: int = 4 # originally 32\n",
        "    n_kv_heads: int = 2 # originally 8\n",
        "    window_size: int = 1024 # originally 4096\n",
        "    context_len: int = 2048 # originally 8192\n",
        "    vocab_size: int = 32001"
      ],
      "metadata": {
        "id": "irns4Yn7rPVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other parts of the model"
      ],
      "metadata": {
        "id": "pBIA31MI_AwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, args: ModelArguments):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(args.vocab_size, args.dim)\n",
        "        self.positional_encoding = nn.Embedding(args.context_len, args.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) + self.positional_encoding(torch.arange(x.shape[1], device = x.device))\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, args: ModelArguments):\n",
        "        super().__init__()\n",
        "        self.g_val = nn.Parameter(torch.ones(args.dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        rms = torch.sqrt(reduce(x**2, 'batch token dim -> batch token 1', 'mean'))\n",
        "        return x * self.g_val / rms\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(dim, hidden_dim)\n",
        "        self.lin2 = nn.Linear(dim, hidden_dim)\n",
        "        self.lin3 = nn.Linear(hidden_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.silu(self.lin1(x)) + self.lin2(x)\n",
        "        return self.lin3(x)\n"
      ],
      "metadata": {
        "id": "TQuVvha8_AJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "\n",
        "- **Grouped-Query Attention**: Same key and value for heads on the same group.\n",
        "- **Sliding Window Attention**: Slides a window of certain size and apply full GQA on the tokens of that window. This composes over all layers, virtually increasing the effective context."
      ],
      "metadata": {
        "id": "-j-qeETPsp4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouped-Query Attention"
      ],
      "metadata": {
        "id": "Fc_0ERL6s5sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(seq_len, window_size):\n",
        "    idx = torch.arange(seq_len)\n",
        "    diff = rearrange(idx, 'i->i 1') - rearrange(idx, 'i->1 i')\n",
        "    mask = ((diff >= 0) & (diff < window_size))\n",
        "    return mask.float()"
      ],
      "metadata": {
        "id": "xvbJyRXkIu3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupQueryAttention(nn.Module):\n",
        "    def __init__(self, args: ModelArguments):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.head_dim\n",
        "        self.n_heads = args.n_heads\n",
        "        self.n_kv_heads = args.n_kv_heads\n",
        "        self.window_size = args.window_size\n",
        "\n",
        "        self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim)\n",
        "        self.wk = nn.Linear(self.dim, self.n_heads * self.head_dim // self.n_kv_heads)\n",
        "        self.wv = nn.Linear(self.dim, self.n_heads * self.head_dim // self.n_kv_heads)\n",
        "        self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim)\n",
        "\n",
        "        self.rms_norm = RMSNorm(args)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        mask = create_mask(x.shape[1], self.window_size).to(device)\n",
        "\n",
        "        q = rearrange(self.wq(x), 'batch token_query (head_query dim) -> head_query batch token_query dim', head_query = self.n_heads)\n",
        "        k = repeat(self.wk(x), 'batch token_key (head_group dim) -> (group head_group) batch token_key dim', head_group = self.n_heads // self.n_kv_heads, group = self.n_kv_heads)\n",
        "        v = repeat(self.wv(x), 'batch token_key (head_group dim) -> (group head_group) batch token_key dim', head_group = self.n_heads // self.n_kv_heads, group = self.n_kv_heads)\n",
        "\n",
        "        att_score = einsum(q, k, 'head batch i dim, head batch j dim -> head batch i j') / np.sqrt(self.head_dim)\n",
        "        att_score = att_score.masked_fill(mask == 0, -1e9)\n",
        "        att_score = att_score.softmax(dim = -1)\n",
        "\n",
        "        output = einsum(att_score, v, 'head batch i j, head batch j dim -> head batch i dim')\n",
        "        output = rearrange(output, 'head batch token dim -> batch token (head dim)')\n",
        "        output = self.wo(output)\n",
        "        output = self.rms_norm(output) + x\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "dLT12jALsOY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "4CY1rJ7ZMlyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mistral(nn.Module):\n",
        "    def __init__(self, args: ModelArguments):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = args.n_layers\n",
        "        self.embedding = Embedding(args)\n",
        "\n",
        "        self.att_layers = nn.ModuleList([GroupQueryAttention(args) for _ in range(self.n_layers)])\n",
        "\n",
        "        self.ff = FeedForward(args.dim, args.hidden_dim)\n",
        "\n",
        "        self.linear = nn.Linear(args.dim, args.vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        for layer in self.att_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x + self.ff(x)\n",
        "        return self.linear(x)\n"
      ],
      "metadata": {
        "id": "rTZAcN62H6Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Mistral(ModelArguments())\n",
        "model"
      ],
      "metadata": {
        "id": "KlZz4lyNsW7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3lnqyH2OB2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "print(batch['input_ids'].shape)\n",
        "y = model(batch['input_ids'])\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "q-uIgGLJAPRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting the number of parameters"
      ],
      "metadata": {
        "id": "QLQNnul8BnnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{round(total_params / 1e6)}M parameters\")"
      ],
      "metadata": {
        "id": "5AmW09QvAku-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = batch['labels']\n",
        "y_true.shape, y.shape"
      ],
      "metadata": {
        "id": "PL_PzhUsEzBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "m1Q1Px9GfJyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Mistral(ModelArguments())\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "E3pf_dcmMNn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)"
      ],
      "metadata": {
        "id": "LP-yYRP-nXu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "IPFHMqGXMMSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, loss_fn, optimizer, epoch):\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    n_batch = 0\n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        x, y = batch['input_ids'], batch['labels']\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        y_pred = model(x)\n",
        "\n",
        "        y_pred = rearrange(y_pred, 'batch token vocab -> (batch token) vocab')\n",
        "        y = rearrange(y, 'batch token -> (batch token)')\n",
        "\n",
        "        loss_value = loss_fn(y_pred, y)\n",
        "\n",
        "        avg_loss += loss_value\n",
        "        n_batch += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return avg_loss / n_batch\n",
        "\n",
        "def test(model, test_loader, loss_fn, epoch):\n",
        "\n",
        "    model.eval()\n",
        "    avg_loss = 0\n",
        "    n_batch = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(test_loader):\n",
        "            x, y = batch['input_ids'], batch['labels']\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            y_pred = rearrange(y_pred, 'batch token vocab -> (batch token) vocab')\n",
        "            y = rearrange(y, 'batch token -> (batch token)')\n",
        "\n",
        "            loss_value = loss_fn(y_pred, y)\n",
        "\n",
        "            avg_loss += loss_value\n",
        "            n_batch += 1\n",
        "\n",
        "    return avg_loss / n_batch"
      ],
      "metadata": {
        "id": "woAU1KO6osMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5"
      ],
      "metadata": {
        "id": "kabPFzxsG8-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter()\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    loss = train(model, train_loader, loss_fn, optimizer, epoch)\n",
        "    writer.add_scalar(\"Training Loss\", loss, epoch)\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "CQ2VpNGLrCfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post training"
      ],
      "metadata": {
        "id": "MMbFmuoXfL5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G7JA-eA0e99v"
      }
    }
  ]
}